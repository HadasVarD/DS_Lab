---
title: "Data Science Lab - Final Project"
description: "NLP & Text Classification"
author: "Hadas Wardi & Tomer Zipori"
date: last-modified
title-block-banner: "#D70E90"
execute: 
  warning: false
  message: false
  cache: true
format:
  html:
    embed-resources: true
    toc: true
    toc-depth: 3
bibliography: references.bib
editor: visual
---

# pre-processing

### Setup

```{r}
#| output: false
#| include: true
library(factoextra)
library(cluster)
library(tidyverse)
library(text)
library(caret)
library(class)
library(quanteda)
library(quanteda.textplots)
library(pandoc)
```

### Data

```{r}
train <- read_csv("train.csv", show_col_types = F)
test <- read_csv("test.csv", show_col_types = F)
```

### Text cleaning

```{r}
text_clean <- function(data) {
  output <- data |>
    str_remove_all(pattern = "http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+") |>
    str_remove_all(pattern = "[[:punct:]]") |>
    str_trim() |>
    str_squish()
  return(output)
}

train_clean <- train |>
  mutate(content = text_clean(content))
  
test_clean <- test |>
  mutate(content = text_clean(content))
```

### Embedding train and test datasets

```{r}
#| eval: false
train_emb <- textEmbed(train_clean$content, model = "sentence-transformers/all-MiniLM-L6-v2", layers = 6)

test_emb <- textEmbed(test_clean$content, model = "sentence-transformers/all-MiniLM-L6-v2", layers = 6)
```

```{r}
#| echo: false
train_emb <- read_rds("train_emb.rds")
test_emb <- read_rds("test_emb.rds")
```

### Extracting vector representations

```{r}
train_org <- train_emb$texts$texts
test_org <- test_emb$texts$texts

train_vec <- train_emb$texts$texts
test_vec <- test_emb$texts$texts

train_vec$id <- train_clean$rowid
test_vec$id <- test_clean$rowid

train_vec <- train_clean |>
  select(rowid, label) |>
  right_join(train_vec, by = join_by("rowid" == "id")) |>
  select(-rowid) |>
  mutate(label = factor(label))
```

# Data Exploration

plot words cloud from each label:

```{r wordcloud 1}
label0 <- train_clean[train_clean$label == 0,]
label1 <- train_clean[train_clean$label == 1,]

courpuslab0 <- corpus(label0$content)
courpuslab1 <- corpus(label1$content)

dfm_lab_0 <- corpus_subset(courpuslab0) %>% 
    dfm(remove = stopwords('english'), remove_punct = TRUE) %>%
    dfm_trim(min_termfreq = 10, verbose = FALSE)

dfm_lab_1 <- corpus_subset(courpuslab1) %>% 
    dfm(remove = stopwords('english'), remove_punct = TRUE) %>%
    dfm_trim(min_termfreq = 10, verbose = FALSE)

textplot_wordcloud(dfm_lab_0)
textplot_wordcloud(dfm_lab_1)
```

No clear semantic differences...

Based on the Circumplex model of affect, emotions are arranged in a circular pattern in a two dimensional space created by valence (pleasantness) and arousal (intensity) [@russell_circumplex_1980].

![](image.png){width="200," fig-align="center"}

The following is an exploratory analysis based on these two dimensions.

### Clustering texts along the valence dimention: positive and negative emotions

```{r}
#| eval: false
classified_texts <- data.frame("rowid" = train_clean$rowid,
                                "Negative" = rep(NA, length(train_clean$rowid)),
                                "Positive" = rep(NA, length(train_clean$rowid)))

for (i in classified_texts$rowid) {
  tmp <-text::textClassify(train_clean$content[train_clean$rowid == i], return_all_scores = T) |>
  pivot_wider(names_from = label_x, values_from = score_x)
  
  classified_texts$Negative[classified_texts$rowid == i] <- tmp$NEGATIVE
  classified_texts$Positive[classified_texts$rowid == i] <- tmp$POSITIVE
}

classified_texts_positive <- classified_texts |>
  select(-Negative)
```

```{r}
#| echo: false
classified_texts_train <- read_rds("classified_texts_train.rds")
```

### Creating a vector representation of Arousal

We used 49 sentences generated by chatGPT which express high arousal:

```{r}
#| eval: false
arousal_sentences <- c(
  "The thrilling roller coaster ride sent my heart racing with excitement",
  "Bursting with euphoria, I danced wildly at the music festival",
  "The intense passion between them was electric and palpable",
  "My adrenaline surged as I bungee jumped off the towering bridge",
  "The heart-pounding suspense of the horror movie had me on the edge of my seat",
  "I felt an overwhelming rush of exhilaration as I won the race",
  "The electric atmosphere of the concert filled me with energy",
  "The adrenaline-fueled chase left me breathless and on the run",
  "My heart pounded with anticipation before the big game",
  "The intense chemistry between us was undeniable and intoxicating",
  "I felt an overwhelming surge of excitement as I boarded the plane to my dream destination",
  "The high-speed thrill of the motorcycle ride left me exhilarated",
  "I was on cloud nine, feeling invincible after acing the exam",
  "The intense competitiveness of the competition fueled my drive to win",
  "The passionate kiss left me feeling dizzy and on cloud nine",
  "My heart raced with fear as I faced the haunted house alone",
  "The electrifying moment on stage had me shaking with nerves and excitement",
  "I felt an overwhelming surge of joy as I held my newborn baby",
  "The heart-stopping moment during the near-miss accident shook me to my core",
  "The intense attraction between us ignited a fiery desire",
  "My excitement reached a fever pitch as the roller coaster climbed higher",
  "The adrenaline rush of skydiving was unlike anything I had experienced",
  "The intense emotion of the moment left me feeling alive and on fire",
  "My heart pounded with nervousness before going on stage to perform",
  "The passionate embrace and fiery kiss left me weak in the knees",
  "I felt an overwhelming surge of love as I held my partner close",
  "The high-stakes poker game had my heart pounding in my chest",
  "The electrifying touch of their hand sent tingles through my body",
  "I was on cloud nine after the exhilarating dance performance",
  "The intense competition brought out the best in me, pushing me to new limits",
  "My excitement soared as I saw my favorite band take the stage",
  "The adrenaline-fueled adventure sport had me on an emotional high",
  "The heart-pounding moment before the big announcement had me on edge",
  "I was filled with euphoria as I accomplished my long-standing goal",
  "The intense chemistry between us made every moment feel electric",
  "My heart raced with exhilaration as I raced down the ski slope",
  "The thrilling plot twist in the movie had me gasping in surprise",
  "I felt an overwhelming surge of joy as I reunited with an old friend",
  "The heart-stopping moment on the roller coaster made me scream with excitement",
  "The passionate argument left me feeling heated and on edge",
  "My adrenaline surged as I leaped from the high diving board into the pool",
  "The electrifying energy of the live concert had me dancing all night",
  "I was filled with euphoria after the adrenaline-fueled adventure",
  "The intense emotion of the moment left me feeling alive and invigorated",
  "My heart pounded with excitement as I approached the finish line",
  "The electrifying atmosphere of the stadium had me cheering at the top of my lungs",
  "I felt an overwhelming surge of happiness as I reunited with my family",
  "The heart-pounding fear of the haunted house had me clinging to my friend",
  "The intense chemistry between us made every touch feel electric"
)

arousal_emb <- textEmbed(arousal_sentences, model = "sentence-transformers/all-MiniLM-L6-v2", layers = 6)

arousal_vec <- arousal_emb$texts$texts |>
  summarise_all(mean)
```

```{r}
#| echo: false
arousal_vec <- read_rds("arousal_vec.rds")
```

After calculating vector embeddings of all the sentences using the transformesr-based sentence-transformers/all-MiniLM-L6-v2" model from \[HuggingFace\] (https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2), the 49 vectors were averaged to create a mean vector representative of general "Arousal".

In the next step cosine similarity between each document to this vector was calculated and regarded as the document's arousal score.

Calculating cosine similarity between each sentence and the mean arousal embedding:

```{r}
#| eval: false
main <- classified_texts_train |> mutate(similarity = NA)

for( i in c(1:10980)){
  main$similarity[i] = textSimilarity(train_org[i,], arousal_vec)
}
```

```{r}
#| echo: false
main <- read_rds("main.rds")
```

```{r}
#| eval: false
main1 <- main |>
  left_join(train_clean, by = join_by("rowid" == "rowid")) |>
  mutate(label = factor(train_clean$label))
```

## K-means clustering

First step of the analysis is checking if the documents are clustered in some meaningful way on the Valence X Arousal 2d-space. A K-means clustering technique is used, and clusters are compared with actual labels.

```{r}
main <- main |> select(-Negative)
```

```{r}
train_km <- main |> select(-rowid, -similarity, -content, -rowid)
kmeans_mat <- kmeans(train_km, 2, iter.max = 100, nstart = 1)

main <- main |> mutate(kmeans = factor(kmeans_mat$cluster))
ggplot(main, aes(Positive, similarity, color = kmeans)) +
    geom_point(alpha = 0.25) +
    xlab("Positive") +
    ylab("arousal")+
    labs(title = "K-means clusters") +
    guides(color = guide_legend(NULL)) +
    theme_classic() +
    theme(plot.title = element_text(family = "serif", size = 18, hjust = 0.5))
```

Visualizing common words in each cluster:

```{r}
cluster1 <- main[main$kmeans ==1,]
cluster2 <- main[main$kmeans ==2,]

courpus1 <- corpus(cluster1$content)
courpus2 <- corpus(cluster2$content)

dfm_1 <- corpus_subset(courpus1) %>% 
    dfm(remove = stopwords('english'), remove_punct = TRUE) %>%
    dfm_trim(min_termfreq = 10, verbose = FALSE)

dfm_2 <- corpus_subset(courpus2) %>% 
    dfm(remove = stopwords('english'), remove_punct = TRUE) %>%
    dfm_trim(min_termfreq = 10, verbose = FALSE)

textplot_wordcloud(dfm_1)
textplot_wordcloud(dfm_2)
```

Are those clusters correlate with the true label?

```{r}
ggplot(main, aes(Positive, similarity, colour = label)) + geom_point() + theme_classic()
```

Not really...

# Machine Learning

## K Nearest Neighbors

Switching our strategy, we try to predict the label of each document with a K Nearest Neighbors model. Each document is labeled according to the K nearest document in the two-dimensional space (Valence and Arousal).

```{r}
main.knn <- main |> select(Positive, similarity, label)

main.indxTrain <- createDataPartition(y = main.knn$label, p = 0.8, list = FALSE)
main.train <- main[main.indxTrain,]
main.test <- main[-main.indxTrain,]

tcKNN1 <- trainControl(method = "cv", number = 10)
tgKNN1 <- expand.grid(k = c(15:30)) 

set.seed(100)  

knnmain1 <- train(
  label~ Positive + similarity,                     
  data = main.train, 
  method = "knn",                    
  tuneGrid = tgKNN1,                     
  trControl = tcKNN1,                    
  preProcess = c("center", "scale"), 
  metric = "Kappa"
)
```

```{r}
head(knnmain1$results)
```

What is the best value of K?

```{r}
knnmain1$bestTune
```

### Performance

```{r}
main.test <- main.test |> mutate(knn = predict(knnmain1, newdata = main.test, type = "raw"))

(knn_con_mat <- confusionMatrix(main.test$knn, main.test$label, positive = "1"))
```

### Real Test Predictions

```{r}
knn_test_pred <- predict(knnmain1, newdata = main.test, type = "raw")
```

Not very impressive...

As evident from the performance measures, a significant portion of the predictions align with the base rate, indicating that the model's predictions for label 1 were not sufficiently accurate.

## Logistic regression

Training a logistic regression model for predicting the document's label. This time vectorization of texts was evaluated with a transformers-based model (384 latent features).

Re-coding labels:

```{r}
train_vec <- train_vec |>
  mutate(label = factor(case_when(label == "0" ~ "no",
                           label == "1" ~ "yes")))
```

### Data Splitting

In order to evaluate model performance, a test set is split from the large train set.

```{r}
set.seed(14)
train_index <- createDataPartition(train_vec$label, p = 0.8, list = F)

train_vec2 <- train_vec[train_index,]
test_vec2 <- train_vec[-train_index,]
```

Logistic model's performance:

```{r}
glm_model <- glm(label ~ ., data = train_vec, family = binomial())

performance::model_performance(glm_model)
```

```{r}
#| eval: false
glm_model <- train(label ~ .,
                     data = train_vec2,
                     method = "glm",
                     family = "binomial")
```

```{r}
#| echo: false
glm_model <- read_rds("glmcv_model.rds")
```

### Performance

#### Train

```{r}
glm_model
```

#### Test

```{r}
glm_pred <- predict(glm_model, test_vec2, type = "raw")

(glm_con_mat <- confusionMatrix(glm_pred, test_vec2$label))
```

Accuracy and Kappa measures are again not very high compared with the base-rate.

### Real Test Predictions

```{r}
glm_test_pred <- predict(glm_model, newdata = test_vec, type = "raw")

glm_test_pred <- ifelse(glm_test_pred == "yes", 1, 0)
```

## Boosted Random Forest

Fitting a boosted random forest for predicting labels.

```{r}
#| eval: false
library(gbm)
```

### Train control

Fitting a boosted random forest for predicting labels. This process is using adaptive cross-validation for hyper-parameter search [@kuhn_futility_2014].

```{r}
#| eval: false
tc <- trainControl(method = "adaptive_cv",
                   number = 10, repeats = 10,
                   adaptive = list(min = 5, alpha = 0.05, 
                                   method = "BT", complete = F),
                   search = "random",
                   classProbs = T)
```

```{r}
#| eval: false
set.seed(14)
boost_model <- train(label ~ ., 
                   data = train_vec2,
                   method = "gbm",
                   metric = "Kappa",
                   trControl = tc,
                   verbose = T)
```

```{r}
#| echo: false
boost_model <- read_rds("boost_model_mini.rds")
```

### Performance

#### Train

```{r}
boost_model
```

#### Test

```{r}
boost_pred <- predict(boost_model, newdata = test_vec2)

(boost_con_mat <- confusionMatrix(boost_pred, reference = test_vec2$label, positive = "yes"))
```

### Real Test Predictions

```{r}
boost_test_pred <- predict(boost_model, newdata = test_vec, type = "raw")

boost_test_pred <- ifelse(boost_test_pred == "yes", 1, 0)

boost_pred_submission <- data.frame("rowid" = test_clean$rowid,
                                    "label" = boost_test_pred)
```

### Link to shinyapps.io

We also created an interactive [app](https://tomerzipori.shinyapps.io/Text_Classification_Tomer_Hadas/) that summaries performances of the 3 models.
